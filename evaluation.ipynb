{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jiwer in c:\\users\\mrami\\anaconda3\\envs\\ml\\lib\\site-packages (4.0.0)\n",
            "Requirement already satisfied: click>=8.1.8 in c:\\users\\mrami\\anaconda3\\envs\\ml\\lib\\site-packages (from jiwer) (8.2.1)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in c:\\users\\mrami\\anaconda3\\envs\\ml\\lib\\site-packages (from jiwer) (3.14.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\mrami\\appdata\\roaming\\python\\python310\\site-packages (from click>=8.1.8->jiwer) (0.4.6)\n",
            "Requirement already satisfied: editdistance in c:\\users\\mrami\\anaconda3\\envs\\ml\\lib\\site-packages (0.8.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install jiwer\n",
        "!pip install editdistance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mrami\\anaconda3\\envs\\ml\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
            "  warnings.warn(\n",
            "c:\\Users\\mrami\\anaconda3\\envs\\ml\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\mrami\\anaconda3\\envs\\ml\\lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "from GE2PE import GE2PE\n",
        "with open(r\"prompt_base.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "replacements = {\n",
        "    'a': 'A',\n",
        "    '$': 'S',\n",
        "    '/': 'a',\n",
        "    '1': '',\n",
        "    ';': 'Z',\n",
        "    '@': '?',\n",
        "    'c': 'C'\n",
        "}\n",
        "g2p = GE2PE(model_path='homo-ge2pe') \n",
        "base_prompt= \"\"\n",
        "for line in lines:\n",
        "    base_prompt+=line.strip()\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=\"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\",\n",
        ")\n",
        "\n",
        "def phonemize(text):\n",
        "    model_output = g2p.generate([text])[0]\n",
        "    model_output = [\n",
        "    ''.join(replacements.get(char, char) for char in model_output)]\n",
        "    completion = client.chat.completions.create(\n",
        "    \n",
        "        model=\"google/gemini-2.5-flash\",\n",
        "    \n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": base_prompt\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"pinglish text : {model_output} , original not phonemized text : {text}\"\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    return completion.choices[0].message.content\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "گَرْ بَرْ سَرْ نَفْسِ خُوْدْ اَمیری مَرْدی، بَرْ کُوْرْ و کَرْ اَرْ نُکْتِه نَگیری مَرْدی\n"
          ]
        }
      ],
      "source": [
        "test_input = \"گر بر سر نفس خود امیری مردی، بر کور و کر ار نکته نگیری مردی\"\n",
        "output_test = phonemize(test_input)\n",
        "print(output_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "اَگَرْ عِلْمِ اِنْسانْ اَزْ ایْمانْ جُدا شَوَدْ عَواقِبِ خَطَرْناکیْ خواهَدْ داشْتْ\n"
          ]
        }
      ],
      "source": [
        "test_input = \"اگر علم انسان از ایمان جدا شود عواقب خطرناکی خواهد داشت\"\n",
        "output_test = phonemize(test_input)\n",
        "print(output_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "اِعْتِمادِ مُتَقابِلِ بَیْنَ اَقْوامِ مُخْتَلِفْ اَساسِ وَحْدَتِ مِلّی اَسْتْ\n"
          ]
        }
      ],
      "source": [
        "test_input = \"اعتماد متقابل بین اقوام مختلف اساس وحدت ملی است\"\n",
        "output_test = phonemize(test_input)\n",
        "print(output_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "چُوْ خُوْرْشیْدْ اَزْ خاوَرانْ بَرآمَدْ جَهانْ زِ پَرْتَوْ اُوْ رُوْشَنْ گَشْتْ\n"
          ]
        }
      ],
      "source": [
        "test_input = \"چو خورشید از خاوران برآمد جهان ز پرتو او روشن گشت\"\n",
        "output_test = phonemize(test_input)\n",
        "print(output_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "شَهْرِ رَمَضانْ ماهِ بَخْشِشْ وَ مَغْفِرَتْ اَسْتْ\n"
          ]
        }
      ],
      "source": [
        "test_input = \"شهر رمضان ماه بخشش و مغفرت است\"\n",
        "output_test = phonemize(test_input)\n",
        "print(output_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "مَسْئُوْلِیَّتِ اجْتِماعِیِّ شِرْکَت‌ها دَرْ اِقْتِصادِ مُدِرْنْ بِسْیارْ مُهِمْ اَسْتْ\n"
          ]
        }
      ],
      "source": [
        "test_input = \"مسئولیت اجتماعی شرکت‌ها در اقتصاد مدرن بسیار مهم است\"\n",
        "output_test = phonemize(test_input)\n",
        "print(output_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "تَغییراتِ سَریعِ آبُ‌هَوا چالِشْ‌هایِ بُزُرْگی بَرایِ کِشاوَرْزی ایْجادْ کَرْدِه اَسْتْ\n"
          ]
        }
      ],
      "source": [
        "test_input = \"تغییرات سریع آب‌وهوا چالش‌های بزرگی برای کشاورزی ایجاد کرده است\"\n",
        "output_test = phonemize(test_input)\n",
        "print(output_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "مَسْعودْ نِوِشْتِه‌اِیْ بِه مَنْ دادْ وَ مَنْ را بِه فِکْرْ فُرُوْ بُرْدْ\n"
          ]
        }
      ],
      "source": [
        "test_input = \"مسعود نوشته ای به من داد و من را به فکر فرو برد\"\n",
        "output_test = phonemize(test_input)\n",
        "print(output_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "خْواسْتَنْ تَوانِسْتَنْ اَسْتْ\n"
          ]
        }
      ],
      "source": [
        "test_input = \"خواستن توانستن است\"\n",
        "output_test = phonemize(test_input)\n",
        "print(output_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "گُفْت‌وُگو مَیانِ فَرْهَنْگ‌ها راهی بَرایِ صُلْحِ پایْدارْ اَسْتْ\n"
          ]
        }
      ],
      "source": [
        "test_input = \"گفت‌ و گو میان فرهنگ‌ها راهی برای صلح پایدار است\"\n",
        "output_test = phonemize(test_input)\n",
        "print(output_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "دِلْ می‌رَوَدْ زِه‌ دَسْتَمْ صاحِبْدِلانْ خُدا را\n"
          ]
        }
      ],
      "source": [
        "test_input = \"دل می‌رود ز دستم صاحب‌دلان خدا را\"\n",
        "output_test = phonemize(test_input)\n",
        "print(output_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "مَسْأَلَه‌یِ عَدالَتْ دَرْ توزیعِ ثِرْوَتْ اَزْ دیدْگاه‌هایِ مُخْتَلِفی مُوْرِدِ تَوَجُّهْ اَسْتْ\n"
          ]
        }
      ],
      "source": [
        "test_input = \"مسئله عدالت در توزیع ثروت از دیدگاه‌های مختلفی مورد توجه است\"\n",
        "output_test = phonemize(test_input)\n",
        "print(output_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ژالِه وَ ژینوسْ باهَمْ بِه اِسْکی رَفْتَنْدْ وَ مُرْدَنْدْ\n"
          ]
        }
      ],
      "source": [
        "test_input = \"ژاله و ژینوس باهم به اسکی رفتند و مردند\"\n",
        "output_test = phonemize(test_input)\n",
        "print(output_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "آنْ‌ها هَمِگی مَرْدْ هَسْتَنْدْ وَ زَنْ نِیْسْتَنْدْ\n"
          ]
        }
      ],
      "source": [
        "test_input = \"آنها همگی مرد هستند  و زن نیستند\"\n",
        "output_test = phonemize(test_input)\n",
        "print(output_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "او مَرْدْ اَسْتْ\n"
          ]
        }
      ],
      "source": [
        "test_input = \"او مرد است\"\n",
        "output_test = phonemize(test_input)\n",
        "print(output_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "اوْ فَرْمانْدِهْیِ یِکْ گُرْدانْ بودْ\n"
          ]
        }
      ],
      "source": [
        "test_input = \"او فرمانده یک گردان بود\"\n",
        "output_test = phonemize(test_input)\n",
        "print(output_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "شَخْصاً بِهْ دیْدٰارِ اوْ رَفْتَمْ\n"
          ]
        }
      ],
      "source": [
        "test_input = \"شخصا به دیدار او رفتم\"\n",
        "output_test = phonemize(test_input)\n",
        "print(output_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"Untitled spreadsheet - Sheet1(1).csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df['model_predict'] = test_df['column 1'].apply(phonemize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_temp = test_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "SUKUN = '\\u0652'\n",
        "TASHDEED = '\\u0651'\n",
        "DOT = '\\u002E'  \n",
        "PERSIAN_COMMA = '\\u060C'\n",
        "FATHA = '\\u064E'\n",
        "KASRA = '\\u0650'\n",
        "DAMMA = '\\u064F'\n",
        "for col in ['column 2', 'model_predict']:\n",
        "    test_temp[col] = test_temp[col].str.replace(SUKUN, '', regex=False)\n",
        "    test_temp[col] = test_temp[col].str.replace(TASHDEED, '', regex=False) \n",
        "    test_temp[col] = test_temp[col].str.replace(DOT, '', regex=False) \n",
        "    test_temp[col] = test_temp[col].str.replace(PERSIAN_COMMA, '', regex=False)       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phoneme Error Rate: 0.07415437987857762\n",
            "Word Error Rate: 0.30982367758186397\n"
          ]
        }
      ],
      "source": [
        "from jiwer import wer\n",
        "import editdistance\n",
        "\n",
        "refs = list(test_temp['column 2'].values)\n",
        "hyps = list(test_temp['model_predict'].values)\n",
        "\n",
        "total_edits = 0\n",
        "total_length = 0\n",
        "\n",
        "for ref, hyp in zip(refs, hyps):\n",
        "    total_edits += editdistance.eval(list(ref), list(hyp))\n",
        "    total_length += len(ref)\n",
        "\n",
        "PER = total_edits / total_length\n",
        "print(\"Phoneme Error Rate:\", PER)\n",
        "print(\"Word Error Rate:\", wer(refs, hyps))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_bleu(reference, hypothesis):\n",
        "    ref_tokens = list(reference)\n",
        "    hyp_tokens = list(hypothesis)\n",
        "    smooth = SmoothingFunction().method1\n",
        "    return sentence_bleu([ref_tokens], hyp_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 351,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU score: 0.8566\n"
          ]
        }
      ],
      "source": [
        "test_temp['BLEU'] = [\n",
        "    compute_bleu(ref, hyp)\n",
        "    for ref, hyp in zip(test_temp['column 2'], test_temp['model_predict'])\n",
        "]\n",
        "mean_bleu = test_temp['BLEU'].mean()\n",
        "print(f\"Average BLEU score: {mean_bleu:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "fatha_test = test_df.copy()\n",
        "for col in ['column 2', 'model_predict']:\n",
        "    fatha_test[col] = fatha_test[col].str.replace(SUKUN, '', regex=False)\n",
        "    fatha_test[col] = fatha_test[col].str.replace(TASHDEED, '', regex=False) \n",
        "    fatha_test[col] = fatha_test[col].str.replace(DOT, '', regex=False) \n",
        "    fatha_test[col] = fatha_test[col].str.replace(PERSIAN_COMMA, '', regex=False)\n",
        "    fatha_test[col] = fatha_test[col].str.replace(DAMMA, '', regex=False)\n",
        "    fatha_test[col] = fatha_test[col].str.replace(KASRA, '', regex=False)     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phoneme Error Rate: 0.0361504640937958\n",
            "Word Error Rate: 0.1561712846347607\n"
          ]
        }
      ],
      "source": [
        "from jiwer import wer\n",
        "import editdistance\n",
        "\n",
        "refs = list(fatha_test['column 2'].values)\n",
        "hyps = list(fatha_test['model_predict'].values)\n",
        "\n",
        "total_edits = 0\n",
        "total_length = 0\n",
        "\n",
        "for ref, hyp in zip(refs, hyps):\n",
        "    total_edits += editdistance.eval(list(ref), list(hyp))\n",
        "    total_length += len(ref)\n",
        "\n",
        "PER = total_edits / total_length\n",
        "print(\"Phoneme Error Rate:\", PER)\n",
        "print(\"Word Error Rate:\", wer(refs, hyps))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU score: 0.9289\n"
          ]
        }
      ],
      "source": [
        "fatha_test['BLEU'] = [\n",
        "    compute_bleu(ref, hyp)\n",
        "    for ref, hyp in zip(fatha_test['column 2'], fatha_test['model_predict'])\n",
        "]\n",
        "mean_bleu = fatha_test['BLEU'].mean()\n",
        "print(f\"Average BLEU score: {mean_bleu:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "kasra_test = test_df.copy()\n",
        "for col in ['column 2', 'model_predict']:\n",
        "    kasra_test[col] = kasra_test[col].str.replace(SUKUN, '', regex=False)\n",
        "    kasra_test[col] = kasra_test[col].str.replace(TASHDEED, '', regex=False) \n",
        "    kasra_test[col] = kasra_test[col].str.replace(DOT, '', regex=False) \n",
        "    kasra_test[col] = kasra_test[col].str.replace(PERSIAN_COMMA, '', regex=False)\n",
        "    kasra_test[col] = kasra_test[col].str.replace(DAMMA, '', regex=False)\n",
        "    kasra_test[col] = kasra_test[col].str.replace(FATHA, '', regex=False) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phoneme Error Rate: 0.05606060606060606\n",
            "Word Error Rate: 0.23425692695214106\n"
          ]
        }
      ],
      "source": [
        "refs = list(kasra_test['column 2'].values)\n",
        "hyps = list(kasra_test['model_predict'].values)\n",
        "\n",
        "total_edits = 0\n",
        "total_length = 0\n",
        "\n",
        "for ref, hyp in zip(refs, hyps):\n",
        "    total_edits += editdistance.eval(list(ref), list(hyp))\n",
        "    total_length += len(ref)\n",
        "\n",
        "PER = total_edits / total_length\n",
        "print(\"Phoneme Error Rate:\", PER)\n",
        "print(\"Word Error Rate:\", wer(refs, hyps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU score: 0.8873\n"
          ]
        }
      ],
      "source": [
        "kasra_test['BLEU'] = [\n",
        "    compute_bleu(ref, hyp)\n",
        "    for ref, hyp in zip(kasra_test['column 2'], kasra_test['model_predict'])\n",
        "]\n",
        "mean_bleu = kasra_test['BLEU'].mean()\n",
        "print(f\"Average BLEU score: {mean_bleu:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "damma_test = test_df.copy()\n",
        "for col in ['column 2', 'model_predict']:\n",
        "    damma_test[col] = damma_test[col].str.replace(SUKUN, '', regex=False)\n",
        "    damma_test[col] = damma_test[col].str.replace(TASHDEED, '', regex=False) \n",
        "    damma_test[col] = damma_test[col].str.replace(DOT, '', regex=False) \n",
        "    damma_test[col] = damma_test[col].str.replace(PERSIAN_COMMA, '', regex=False)\n",
        "    damma_test[col] = damma_test[col].str.replace(KASRA, '', regex=False)\n",
        "    damma_test[col] = damma_test[col].str.replace(FATHA, '', regex=False) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phoneme Error Rate: 0.04199475065616798\n",
            "Word Error Rate: 0.181360201511335\n"
          ]
        }
      ],
      "source": [
        "refs = list(damma_test['column 2'].values)\n",
        "hyps = list(damma_test['model_predict'].values)\n",
        "\n",
        "total_edits = 0\n",
        "total_length = 0\n",
        "\n",
        "for ref, hyp in zip(refs, hyps):\n",
        "    total_edits += editdistance.eval(list(ref), list(hyp))\n",
        "    total_length += len(ref)\n",
        "\n",
        "PER = total_edits / total_length\n",
        "print(\"Phoneme Error Rate:\", PER)\n",
        "print(\"Word Error Rate:\", wer(refs, hyps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU score: 0.9144\n"
          ]
        }
      ],
      "source": [
        "damma_test['BLEU'] = [\n",
        "    compute_bleu(ref, hyp)\n",
        "    for ref, hyp in zip(damma_test['column 2'], damma_test['model_predict'])\n",
        "]\n",
        "mean_bleu = damma_test['BLEU'].mean()\n",
        "print(f\"Average BLEU score: {mean_bleu:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
